{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Taxi trips within and outside Manhattan\n",
    "### (Naive Bayes classifier with discrete-valued inputs)\n",
    "\n",
    "Below we apply the Naive Bayes Classifier to predict whether a taxi trip happened within or outside Manhattan. We are given a sample of workday daytime taxi trips with speed and distance information, as well as the number of passengers and the size of the tip.  Speed, distance, and tip information were encoded as discrete categorical variables, with larger values corresponding to faster speeds, longer distances, and higher tips respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data1 = pd.read_csv(\"NYC_taxi_sample.csv\")\n",
    "data1_X = data1.iloc[:,1:] # tip, distance, speed, and number of passengers\n",
    "data1_y = data1.iloc[:,0] # binary output: 1 if in Manhattan, 0 if outside\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1_X, data1_y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple implementation of the Naive Bayes estimator for the training sample statistics $P(x=x^*\\:|\\:y=b)$ and $P(y=b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a binary Naive Bayes Classifier with discrete input attributes.\n",
    "# Assume that the binary output variable takes on values 0 or 1. \n",
    "def trainNaiveBayesDiscrete(X,y):\n",
    "    prior = 1.*y.sum()/y.count()\n",
    "    nbc = {'prior':prior}\n",
    "    X_1 = X[y==1]\n",
    "    X_0 = X[y==0]\n",
    "    for j in X.columns:\n",
    "        nbc[j+'_1'] = X_1[j].value_counts(normalize=True)\n",
    "        nbc[j+'_0'] = X_0[j].value_counts(normalize=True)\n",
    "    return nbc\n",
    "\n",
    "def testNaiveBayesDiscrete(X,nbc):\n",
    "    y_pred = pd.Series(index=X.index)\n",
    "    for i in X.index:\n",
    "        # compute odds of y=1\n",
    "        y_pred[i] = nbc['prior']/(1-nbc['prior']) # prior odds\n",
    "        for j in X.columns:\n",
    "            thevalue = X.loc[i,j]\n",
    "            if thevalue not in nbc[j+'_1']:\n",
    "                y_pred[i] = y_pred[i]*1E-3\n",
    "            if thevalue not in nbc[j+'_0']:\n",
    "                y_pred[i] = y_pred[i]*1E3\n",
    "            if (thevalue in nbc[j+'_1']) & (thevalue in nbc[j+'_0']):\n",
    "                y_pred[i] = y_pred[i]*(nbc[j+'_1'][thevalue]+1E-3)/(nbc[j+'_0'][thevalue]+1E-3)\n",
    "        # convert odds to probability of y=1\n",
    "        y_pred[i] = y_pred[i]/(1.0+y_pred[i])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior\n",
      "0.4844844844844845\n",
      "tip_1\n",
      "1    0.368285\n",
      "2    0.175103\n",
      "3    0.169938\n",
      "4    0.166322\n",
      "6    0.062500\n",
      "5    0.057851\n",
      "Name: tip, dtype: float64\n",
      "tip_0\n",
      "1    0.558252\n",
      "4    0.166505\n",
      "3    0.097573\n",
      "2    0.092718\n",
      "5    0.050485\n",
      "6    0.034466\n",
      "Name: tip, dtype: float64\n",
      "dist_1\n",
      "1    0.440083\n",
      "2    0.262397\n",
      "3    0.136364\n",
      "4    0.097624\n",
      "6    0.037190\n",
      "5    0.026343\n",
      "Name: dist, dtype: float64\n",
      "dist_0\n",
      "6    0.241748\n",
      "1    0.208252\n",
      "2    0.177184\n",
      "5    0.141748\n",
      "3    0.119417\n",
      "4    0.111650\n",
      "Name: dist, dtype: float64\n",
      "speed_1\n",
      "2    0.411157\n",
      "1    0.310434\n",
      "3    0.158058\n",
      "4    0.072314\n",
      "5    0.033574\n",
      "6    0.014463\n",
      "Name: speed, dtype: float64\n",
      "speed_0\n",
      "2    0.225243\n",
      "3    0.204854\n",
      "4    0.199029\n",
      "5    0.135437\n",
      "6    0.118932\n",
      "1    0.116505\n",
      "Name: speed, dtype: float64\n",
      "pass_1\n",
      "1    0.645661\n",
      "2    0.196281\n",
      "3    0.094525\n",
      "4    0.063533\n",
      "Name: pass, dtype: float64\n",
      "pass_0\n",
      "1    0.704369\n",
      "2    0.130583\n",
      "4    0.109223\n",
      "3    0.055825\n",
      "Name: pass, dtype: float64\n",
      "In sample prediction accuracy: 0.714964964964965\n",
      "Out of sample prediction accuracy: 0.7014253563390848\n",
      "Log-likelihood (train): -2362.8270145996366\n",
      "Log-likelihood (test): -831.8540075663902\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = trainNaiveBayesDiscrete(X_train,y_train)\n",
    "for i,j in naive_bayes_classifier.items():\n",
    "    print (i)\n",
    "    print (j)\n",
    "    print\n",
    "y_pred_train = testNaiveBayesDiscrete(X_train,naive_bayes_classifier)\n",
    "y_pred_test = testNaiveBayesDiscrete(X_test,naive_bayes_classifier)\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('In sample prediction accuracy:',1.0*sum((y_pred_train>0.5)==y_train)/len(y_train))\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))\n",
    "\n",
    "# measure accuracy of the predicted probabilities\n",
    "print ('Log-likelihood (train):',sum(np.log(y_pred_train*y_train+(1-y_pred_train)*(1-y_train))))\n",
    "print ('Log-likelihood (test):',sum(np.log(y_pred_test*y_test+(1-y_pred_test)*(1-y_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Classification of individual vs. commercial properities\n",
    "### (Gaussian Naive Bayes classifier with real-valued inputs)\n",
    "\n",
    "Same dataset as last week's. Based on the sample of characteristics and the prices of the single unit residential and commercial properties sold in zip code 11201 (downtown Brooklyn) between the years 2009 and 2012, build a classifier defining if the sold property was actually residential or commercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"NYC_individual_commercial.csv\")\n",
    "data2_X = data2.iloc[:,:4]\n",
    "data2_y = data2.iloc[:,4]\n",
    "\n",
    "# reduce correlation between attributes when possible\n",
    "# land --> ratio of land area to inside area; price --> price per square foot\n",
    "data2_X['land']=data2_X['land']/data2_X['area']\n",
    "data2_X['price']=data2_X['price']/data2_X['area']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data2_X, data2_y, test_size=0.33, random_state=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple implementation of the Gaussian Naive Bayes estimator for the training sample statistics $\\mu(x\\:|\\:y=b)$, $\\sigma(x\\:|\\:y=b)$, and $P(y=b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a binary Gaussian Naive Bayes Classifier with real-valued input attributes.\n",
    "# Assume that the binary output variable takes on values 0 or 1. \n",
    "def trainGaussianNaiveBayes(X,y):\n",
    "    prior = 1.*y.sum()/y.count()\n",
    "    nbc = {'prior':prior}\n",
    "    X_1 = X[y==1]\n",
    "    X_0 = X[y==0]\n",
    "    for j in X.columns:\n",
    "        nbc[j+'_mu1'] = X_1[j].mean()\n",
    "        nbc[j+'_sigma1'] = X_1[j].std()\n",
    "        nbc[j+'_mu0'] = X_0[j].mean()\n",
    "        nbc[j+'_sigma0'] = X_0[j].std()\n",
    "    return nbc\n",
    "\n",
    "def testGaussianNaiveBayes(X,nbc):\n",
    "    y_pred = pd.Series(index=X.index)\n",
    "    for i in X.index:\n",
    "        # compute odds of y=1\n",
    "        y_pred[i] = nbc['prior']/(1-nbc['prior']) # prior odds\n",
    "        for j in X.columns:\n",
    "            thevalue = X.loc[i,j]\n",
    "            pdf1 = stats.norm.pdf(thevalue,loc=nbc[j+'_mu1'],scale=nbc[j+'_sigma1'])\n",
    "            pdf0 = stats.norm.pdf(thevalue,loc=nbc[j+'_mu0'],scale=nbc[j+'_sigma0'])\n",
    "            y_pred[i] = y_pred[i]*pdf1/pdf0 if pdf0 > 0 else 1E10\n",
    "        # convert odds to probability of y=1\n",
    "        y_pred[i] = y_pred[i]/(1.0+y_pred[i])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior\n",
      "0.3870967741935484\n",
      "area_mu1\n",
      "16960.5\n",
      "area_sigma1\n",
      "25506.838239117795\n",
      "area_mu0\n",
      "2902.3947368421054\n",
      "area_sigma0\n",
      "1252.3044604221343\n",
      "land_mu1\n",
      "0.8421320950435431\n",
      "land_sigma1\n",
      "0.8421904378508392\n",
      "land_mu0\n",
      "0.6088170692188355\n",
      "land_sigma0\n",
      "0.28926784621778173\n",
      "year_mu1\n",
      "1935.5416666666667\n",
      "year_sigma1\n",
      "27.719055079362892\n",
      "year_mu0\n",
      "1916.078947368421\n",
      "year_sigma0\n",
      "51.069312507053596\n",
      "price_mu1\n",
      "379.1549524937096\n",
      "price_sigma1\n",
      "313.20656471821735\n",
      "price_mu0\n",
      "901.8079931869711\n",
      "price_sigma0\n",
      "352.9210300773802\n",
      "In sample prediction accuracy: 0.8870967741935484\n",
      "Out of sample prediction accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = trainGaussianNaiveBayes(X_train,y_train)\n",
    "for i,j in naive_bayes_classifier.items():\n",
    "    print (i)\n",
    "    print (j)\n",
    "    print\n",
    "y_pred_train = testGaussianNaiveBayes(X_train,naive_bayes_classifier)\n",
    "y_pred_test = testGaussianNaiveBayes(X_test,naive_bayes_classifier)\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('In sample prediction accuracy:',1.0*sum((y_pred_train>0.5)==y_train)/len(y_train))\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Package from Sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample prediction accuracy: 0.8870967741935484\n",
      "Out of sample prediction accuracy: 0.78125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "trained_model = gnb.fit(X_train,y_train)\n",
    "y_pred_train = trained_model.predict_proba(X_train)[:,1]\n",
    "y_pred_test = trained_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('In sample prediction accuracy:',1.0*sum((y_pred_train>0.5)==y_train)/len(y_train))\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Spam classification\n",
    "\n",
    "1. Title:  SPAM E-mail Database\n",
    "\n",
    "2. Sources:\n",
    "   (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
    "        Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
    "   (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
    "   (c) Generated: June-July 1999\n",
    "\n",
    "3. Past Usage:\n",
    "   (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n",
    "   (b) Determine whether a given email is spam or not.\n",
    "   (c) ~7% misclassification error.\n",
    "       False positives (marking good mail as spam) are very undesirable.\n",
    "       If we insist on zero false positives in the training/testing set,\n",
    "       20-25% of the spam passed through the filter.\n",
    "\n",
    "4. Relevant Information:\n",
    "        The \"spam\" concept is diverse: advertisements for products/web\n",
    "        sites, make money fast schemes, chain letters, pornography...\n",
    "\tThe collection of spam e-mails came from the postmaster and \n",
    "\tindividuals who had filed spam.  The collection of non-spam \n",
    "\te-mails came from filed work and personal e-mails, and hence\n",
    "\tthe word 'george' and the area code '650' are indicators of \n",
    "\tnon-spam.  These are useful when constructing a personalized \n",
    "\tspam filter.  One would either have to blind such non-spam \n",
    "\tindicators or get a very wide collection of non-spam to \n",
    "\tgenerate a general purpose spam filter.\n",
    "\n",
    "        For background on spam:\n",
    "        Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n",
    "        Communications of the ACM, 41(8):74-83, 1998.\n",
    "\n",
    "5. Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "\n",
    "6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    "7. Attribute Information:\n",
    "The last column of 'spambase.data' denotes whether the e-mail was \n",
    "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
    "Most of the attributes indicate whether a particular word or\n",
    "character was frequently occuring in the e-mail.  The run-length\n",
    "attributes (55-57) measure the length of sequences of consecutive \n",
    "capital letters.  For the statistical measures of each attribute, \n",
    "see the end of this file.  Here are the definitions of the attributes:\n",
    "\n",
    "48 continuous real [0,100] attributes of type word_freq_WORD \n",
    "= percentage of words in the e-mail that match WORD,\n",
    "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
    "total number of words in e-mail.  A \"word\" in this case is any \n",
    "string of alphanumeric characters bounded by non-alphanumeric \n",
    "characters or end-of-string.\n",
    "\n",
    "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
    "= percentage of characters in the e-mail that match CHAR,\n",
    "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail\n",
    "\n",
    "1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), \n",
    "i.e. unsolicited commercial e-mail.  \n",
    "\n",
    "\n",
    "8. Missing Attribute Values: None\n",
    "\n",
    "9. Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data = urllib.request.urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\").read()\n",
    "data_name=urllib.request.urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\").read()\n",
    "# Read the data\n",
    "data=data.split(b\"\\r\\n\")\n",
    "data_spam=[]\n",
    "for i in range(len(data)):\n",
    "    if len(data[i])>0:\n",
    "        temp=data[i].split(b\",\")\n",
    "        #change from str to float\n",
    "        t_l=[]\n",
    "        for j in range(len(temp)):\n",
    "            t_l.append(float(temp[j]))\n",
    "        data_spam.append(t_l)\n",
    "\n",
    "#Read the column names\n",
    "temp=data_name.split(b\"\\r\\n\")\n",
    "column_names=[]\n",
    "for i in temp:\n",
    "    if (i.startswith(b'word') or i.startswith(b'char') or i.startswith(b'capital')):\n",
    "        column_names.append(i.split(b\":\")[0])\n",
    "column_names.append(\"spam\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spam=pd.DataFrame(data_spam)\n",
    "data_spam.columns=column_names\n",
    "data_spam_X=data_spam.iloc[:,0:-1]\n",
    "data_spam_y=data_spam.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_spam_X, data_spam_y, test_size=0.2, random_state=2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Use the Gaussian naive Bayes code that we provided to build your model on the training data, and report the out-of-sample accuracy on your testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a binary Gaussian Naive Bayes Classifier with real-valued input attributes.\n",
    "# Assume that the binary output variable takes on values 0 or 1. \n",
    "def trainGaussianNaiveBayes(X,y):\n",
    "    prior = 1.*y.sum()/y.count()\n",
    "    nbc = {'prior':prior}\n",
    "    X_1 = X[y==1]\n",
    "    X_0 = X[y==0]\n",
    "    for j in X.columns:\n",
    "        nbc[j+'_mu1'] = X_1[j].mean()\n",
    "        nbc[j+'_sigma1'] = X_1[j].std()\n",
    "        nbc[j+'_mu0'] = X_0[j].mean()\n",
    "        nbc[j+'_sigma0'] = X_0[j].std()\n",
    "    return nbc\n",
    "\n",
    "def testGaussianNaiveBayes(X,nbc):\n",
    "    y_pred = pd.Series(index=X.index)\n",
    "    for i in X.index:\n",
    "        # compute odds of y=1\n",
    "        y_pred[i] = nbc['prior']/(1-nbc['prior']) # prior odds\n",
    "        for j in X.columns:\n",
    "            thevalue = X.loc[i,j]\n",
    "            pdf1 = stats.norm.pdf(thevalue,loc=nbc[j+'_mu1'],scale=nbc[j+'_sigma1'])\n",
    "            pdf0 = stats.norm.pdf(thevalue,loc=nbc[j+'_mu0'],scale=nbc[j+'_sigma0'])\n",
    "            y_pred[i] = y_pred[i]*pdf1/pdf0 if pdf0 > 0 else 1E10\n",
    "        # convert odds to probability of y=1\n",
    "        y_pred[i] = y_pred[i]/(1.0+y_pred[i])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't concat str to bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-bec89e8db705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnaive_bayes_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainGaussianNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnaive_bayes_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cf60f4afb343>\u001b[0m in \u001b[0;36mtrainGaussianNaiveBayes\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_mu1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mnbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_sigma1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mu0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't concat str to bytes"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = trainGaussianNaiveBayes(X_train,y_train)\n",
    "for i,j in naive_bayes_classifier.items():\n",
    "    print (i)\n",
    "    print (j)\n",
    "    print\n",
    "y_pred_train = testGaussianNaiveBayes(X_train,naive_bayes_classifier)\n",
    "y_pred_test = testGaussianNaiveBayes(X_test,naive_bayes_classifier)\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('In sample prediction accuracy:',1.0*sum((y_pred_train>0.5)==y_train)/len(y_train))\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Use the Sklearn package to double check your solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample prediction accuracy: 0.8176630434782609\n",
      "Out of sample prediction accuracy: 0.8371335504885994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "trained_model = gnb.fit(X_train,y_train)\n",
    "y_pred_train = trained_model.predict_proba(X_train)[:,1]\n",
    "y_pred_test = trained_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('In sample prediction accuracy:',1.0*sum((y_pred_train>0.5)==y_train)/len(y_train))\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised EM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3. Taxi trip classification with partially missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same dataset as before\n",
    "data1 = pd.read_csv(\"NYC_taxi_sample.csv\")\n",
    "data1_X = data1.iloc[:,1:] # tip, distance, speed, and number of passengers\n",
    "data1_y = data1.iloc[:,0] # binary output: 1 if in Manhattan, 0 if outside\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1_X, data1_y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# now let's delete 99% of the labels from the training dataset and see what happens\n",
    "random.seed(2015)\n",
    "Label_index=random.sample(list(range(len(X_train))),int(len(X_train)*0.01))\n",
    "Unlabel_index=[x for x in list(range(len(X_train))) if x not in Label_index]\n",
    "\n",
    "X_train_Labeled=X_train.iloc[Label_index,:]\n",
    "X_train_Unlabeled=X_train.iloc[Unlabel_index,:]   \n",
    "y_train_Labeled=y_train.iloc[Label_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our Naive Bayes Classifier does using only the small sample of labeled training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior\n",
      "0.3333333333333333\n",
      "tip_1\n",
      "1    0.538462\n",
      "5    0.153846\n",
      "3    0.153846\n",
      "4    0.076923\n",
      "2    0.076923\n",
      "Name: tip, dtype: float64\n",
      "tip_0\n",
      "1    0.500000\n",
      "4    0.230769\n",
      "5    0.115385\n",
      "3    0.115385\n",
      "6    0.038462\n",
      "Name: tip, dtype: float64\n",
      "dist_1\n",
      "1    0.538462\n",
      "3    0.307692\n",
      "4    0.076923\n",
      "2    0.076923\n",
      "Name: dist, dtype: float64\n",
      "dist_0\n",
      "2    0.269231\n",
      "1    0.269231\n",
      "6    0.192308\n",
      "5    0.153846\n",
      "3    0.076923\n",
      "4    0.038462\n",
      "Name: dist, dtype: float64\n",
      "speed_1\n",
      "2    0.384615\n",
      "3    0.230769\n",
      "4    0.153846\n",
      "1    0.153846\n",
      "5    0.076923\n",
      "Name: speed, dtype: float64\n",
      "speed_0\n",
      "2    0.346154\n",
      "1    0.192308\n",
      "4    0.153846\n",
      "6    0.115385\n",
      "5    0.115385\n",
      "3    0.076923\n",
      "Name: speed, dtype: float64\n",
      "pass_1\n",
      "1    0.769231\n",
      "4    0.153846\n",
      "3    0.076923\n",
      "Name: pass, dtype: float64\n",
      "pass_0\n",
      "1    0.730769\n",
      "2    0.153846\n",
      "4    0.076923\n",
      "3    0.038462\n",
      "Name: pass, dtype: float64\n",
      "Out of sample prediction accuracy: 0.5656414103525882\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = trainNaiveBayesDiscrete(X_train_Labeled,y_train_Labeled)\n",
    "for i,j in naive_bayes_classifier.items():\n",
    "    print (i)\n",
    "    print (j)\n",
    "    print\n",
    "y_pred_test = testNaiveBayesDiscrete(X_test,naive_bayes_classifier)\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we can do using both labeled and unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeNaiveBayesRandom(X_Unlabeled):\n",
    "    nbc = {'prior':0.5}\n",
    "    for j in X_Unlabeled.columns:\n",
    "        thevalues = X_Unlabeled[j].unique()\n",
    "        nbc[j+'_1'] = {}\n",
    "        nbc[j+'_0'] = {}\n",
    "        for jj in thevalues:\n",
    "            nbc[j+'_1'][jj] = np.random.rand()\n",
    "            nbc[j+'_0'][jj] = np.random.rand()\n",
    "    return nbc\n",
    "    \n",
    "def EM(X_Labeled,y_Labeled,X_Unlabeled,num_iters):\n",
    "\n",
    "    # initialize\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    if len(y_Labeled) > 0:\n",
    "        nbc = trainNaiveBayesDiscrete(X_Labeled,y_Labeled)\n",
    "    else:\n",
    "        nbc = initializeNaiveBayesRandom(X_Unlabeled)\n",
    "    \n",
    "    while True:\n",
    "        t = t + 1\n",
    "        print ('Iteration',t,'of',num_iters)\n",
    "        \n",
    "        # E step - classify with nbc for unlabeled data only\n",
    "        y_pred_Unlabeled = testNaiveBayesDiscrete(X_Unlabeled,nbc)\n",
    "        \n",
    "        # M step\n",
    "        X_for_M_step = pd.concat([X_Labeled,X_Unlabeled]) \n",
    "        y_for_M_step = pd.concat([y_Labeled,y_pred_Unlabeled])\n",
    "        prior = 1.*y_for_M_step.sum()/y_for_M_step.count()\n",
    "        nbc = {'prior':prior}\n",
    "        for j in X_for_M_step.columns:\n",
    "            nbc[j+'_1'] = {}\n",
    "            nbc[j+'_0'] = {}\n",
    "            for theindex in X_for_M_step.index:\n",
    "                current_X = X_for_M_step.loc[theindex,j]\n",
    "                current_y = y_for_M_step.loc[theindex]\n",
    "                if current_X in nbc[j+'_1']:\n",
    "                    nbc[j+'_1'][current_X] += current_y\n",
    "                else:\n",
    "                    nbc[j+'_1'][current_X] = current_y\n",
    "                if current_X in nbc[j+'_0']:\n",
    "                    nbc[j+'_0'][current_X] += (1.0-current_y)\n",
    "                else:\n",
    "                    nbc[j+'_0'][current_X] = 1.0-current_y\n",
    "            # normalize probabilities\n",
    "            tempsum = 0.0\n",
    "            for k in nbc[j+'_1']:\n",
    "                tempsum += nbc[j+'_1'][k]\n",
    "            for k in nbc[j+'_1']:\n",
    "                nbc[j+'_1'][k] /= tempsum\n",
    "            tempsum = 0.0\n",
    "            for k in nbc[j+'_0']:\n",
    "                tempsum += nbc[j+'_0'][k]\n",
    "            for k in nbc[j+'_0']:\n",
    "                nbc[j+'_0'][k] /= tempsum            \n",
    "        print(nbc)               \n",
    "        if t==num_iters:\n",
    "            break\n",
    "            \n",
    "    return nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 1\n",
      "{'prior': 0.3467315734196729, 'tip_1': {5: 0.06397637079926267, 4: 0.0794595525285342, 1: 0.44217047260186293, 3: 0.10210678924541221, 6: 0.0001958301943489802, 2: 0.31209098463057905}, 'tip_0': {5: 0.04878764233560655, 4: 0.21257001730803873, 1: 0.4789788390190559, 3: 0.148834661196939, 6: 0.07344629800616703, 2: 0.037382542134192974}, 'dist_1': {1: 0.4297200671950719, 6: 0.013388467254946294, 3: 0.23467686819405037, 2: 0.16033051677281157, 5: 0.009010145873787566, 4: 0.15287393470933225}, 'dist_0': {1: 0.26263791201444064, 6: 0.21124614738852454, 3: 0.07080971005387386, 2: 0.24932601897393877, 5: 0.1266121710691713, 4: 0.07936804050005082}, 'speed_1': {3: 0.24074805074191286, 6: 0.0007819832900163148, 4: 0.10768547722041129, 2: 0.37148140429828336, 5: 0.044817792135719695, 1: 0.23448529231365647}, 'speed_0': {3: 0.15109750867254348, 6: 0.10416419537982363, 4: 0.15353517574941786, 2: 0.2855043591853232, 5: 0.10798982413854315, 1: 0.19770893687434862}, 'pass_1': {3: 0.08473967951527545, 2: 0.01595268736958396, 1: 0.8199314471690297, 4: 0.0793761859461109}, 'pass_0': {3: 0.06917929954419333, 2: 0.24014800905305164, 1: 0.5994929331007282, 4: 0.09117975830202701}}\n",
      "prior\n",
      "0.3467315734196729\n",
      "\n",
      "tip_1\n",
      "{5: 0.06397637079926267, 4: 0.0794595525285342, 1: 0.44217047260186293, 3: 0.10210678924541221, 6: 0.0001958301943489802, 2: 0.31209098463057905}\n",
      "\n",
      "tip_0\n",
      "{5: 0.04878764233560655, 4: 0.21257001730803873, 1: 0.4789788390190559, 3: 0.148834661196939, 6: 0.07344629800616703, 2: 0.037382542134192974}\n",
      "\n",
      "dist_1\n",
      "{1: 0.4297200671950719, 6: 0.013388467254946294, 3: 0.23467686819405037, 2: 0.16033051677281157, 5: 0.009010145873787566, 4: 0.15287393470933225}\n",
      "\n",
      "dist_0\n",
      "{1: 0.26263791201444064, 6: 0.21124614738852454, 3: 0.07080971005387386, 2: 0.24932601897393877, 5: 0.1266121710691713, 4: 0.07936804050005082}\n",
      "\n",
      "speed_1\n",
      "{3: 0.24074805074191286, 6: 0.0007819832900163148, 4: 0.10768547722041129, 2: 0.37148140429828336, 5: 0.044817792135719695, 1: 0.23448529231365647}\n",
      "\n",
      "speed_0\n",
      "{3: 0.15109750867254348, 6: 0.10416419537982363, 4: 0.15353517574941786, 2: 0.2855043591853232, 5: 0.10798982413854315, 1: 0.19770893687434862}\n",
      "\n",
      "pass_1\n",
      "{3: 0.08473967951527545, 2: 0.01595268736958396, 1: 0.8199314471690297, 4: 0.0793761859461109}\n",
      "\n",
      "pass_0\n",
      "{3: 0.06917929954419333, 2: 0.24014800905305164, 1: 0.5994929331007282, 4: 0.09117975830202701}\n",
      "\n",
      "Out of sample prediction accuracy: 0.5933983495873969\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier=EM(X_train_Labeled,y_train_Labeled,X_train_Unlabeled,num_iters=1)\n",
    "for i,j in naive_bayes_classifier.items():\n",
    "    print (i)\n",
    "    print (j)\n",
    "    print()\n",
    "y_pred_test = testNaiveBayesDiscrete(X_test,naive_bayes_classifier)\n",
    "\n",
    "# measure accuracy for the binary prediction task\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised EM clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 4. Taxi trip clustering with no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Unlabeled=X_train # assume all observations are unlabeled\n",
    "X_train_Labeled=X_train.iloc[[],:] # empty\n",
    "y_train_Labeled=y_train.iloc[[]] # empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 50\n",
      "Iteration 2 of 50\n",
      "Iteration 3 of 50\n",
      "Iteration 4 of 50\n",
      "Iteration 5 of 50\n",
      "Iteration 6 of 50\n",
      "Iteration 7 of 50\n",
      "Iteration 8 of 50\n",
      "Iteration 9 of 50\n",
      "Iteration 10 of 50\n",
      "Iteration 11 of 50\n",
      "Iteration 12 of 50\n",
      "Iteration 13 of 50\n",
      "Iteration 14 of 50\n",
      "Iteration 15 of 50\n",
      "Iteration 16 of 50\n",
      "Iteration 17 of 50\n",
      "Iteration 18 of 50\n",
      "Iteration 19 of 50\n",
      "Iteration 20 of 50\n",
      "Iteration 21 of 50\n",
      "Iteration 22 of 50\n",
      "Iteration 23 of 50\n",
      "Iteration 24 of 50\n",
      "Iteration 25 of 50\n",
      "Iteration 26 of 50\n",
      "Iteration 27 of 50\n",
      "Iteration 28 of 50\n",
      "Iteration 29 of 50\n",
      "Iteration 30 of 50\n",
      "Iteration 31 of 50\n",
      "Iteration 32 of 50\n",
      "Iteration 33 of 50\n",
      "Iteration 34 of 50\n",
      "Iteration 35 of 50\n",
      "Iteration 36 of 50\n",
      "Iteration 37 of 50\n",
      "Iteration 38 of 50\n",
      "Iteration 39 of 50\n",
      "Iteration 40 of 50\n",
      "Iteration 41 of 50\n",
      "Iteration 42 of 50\n",
      "Iteration 43 of 50\n",
      "Iteration 44 of 50\n",
      "Iteration 45 of 50\n",
      "Iteration 46 of 50\n",
      "Iteration 47 of 50\n",
      "Iteration 48 of 50\n",
      "Iteration 49 of 50\n",
      "Iteration 50 of 50\n",
      "prior\n",
      "0.5341637635239567\n",
      "tip_1\n",
      "{5: 0.05386104028421211, 4: 0.16599055483023595, 1: 0.42406581491319334, 3: 0.14941587116447697, 6: 0.05519960357352501, 2: 0.1514671152343567}\n",
      "tip_0\n",
      "{5: 0.05427537853170885, 4: 0.16690474216225212, 1: 0.5145491178478944, 3: 0.11338767666675792, 6: 0.03984752280888322, 2: 0.11103556198250354}\n",
      "dist_1\n",
      "{1: 0.5880288679564248, 6: 0.0010725993678910644, 3: 0.08213804413205232, 2: 0.31054221229194046, 5: 0.0013733876414008962, 4: 0.016844888610290366}\n",
      "dist_0\n",
      "{1: 0.013882254955421636, 6: 0.30497777502726103, 3: 0.1797894931470431, 2: 0.11288961119763492, 5: 0.18268699440023994, 4: 0.20577387127239946}\n",
      "speed_1\n",
      "{3: 0.07914327314678785, 6: 0.0011024575482090444, 4: 0.00749925382155522, 2: 0.5217497208692008, 5: 0.0015470335717398427, 1: 0.3889582610425074}\n",
      "speed_0\n",
      "{3: 0.3003345437419287, 6: 0.14539320933382274, 4: 0.2868643474444838, 2: 0.07860170148041025, 5: 0.18302508507298657, 1: 0.00578111292636787}\n",
      "pass_1\n",
      "{3: 0.0876666056478112, 2: 0.19306991226723752, 1: 0.6447089929120811, 4: 0.07455448917287026}\n",
      "pass_0\n",
      "{3: 0.05956224182185543, 2: 0.12725815814793023, 1: 0.7117216694054224, 4: 0.1014579306247919}\n",
      "Out of sample prediction accuracy: 0.6804201050262566\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier=EM(X_train_Labeled,y_train_Labeled,X_train_Unlabeled,num_iters=50)\n",
    "for i,j in naive_bayes_classifier.items():\n",
    "    print (i)\n",
    "    print (j)\n",
    "    print\n",
    "y_pred_test = testNaiveBayesDiscrete(X_test,naive_bayes_classifier)\n",
    "\n",
    "# check if labels switched\n",
    "if (1.0*sum((y_pred_test>0.5)==y_test)/len(y_test) < 0.5):\n",
    "    y_pred_test = 1.0-y_pred_test\n",
    "print ('Out of sample prediction accuracy:',1.0*sum((y_pred_test>0.5)==y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
